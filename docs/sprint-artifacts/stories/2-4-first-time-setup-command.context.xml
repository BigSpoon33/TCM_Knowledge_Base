<story-context id="{bmad_folder}/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>2.4</storyId>
    <title>First Time Setup Command</title>
    <status>drafted</status>
    <generatedAt>2025-11-16</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>/home/shuma/Documents/AI_Suite/Obsidian_Capsule_Delivery/docs/sprint-artifacts/2-4-first-time-setup-command.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>a new user</asA>
    <iWant>a `capsule init` command that interactively guides me through creating my initial `config.yaml` file</iWant>
    <soThat>I can quickly and easily set up the CLI for the first time without manually creating the file</soThat>
    <tasks>
      <task id="3.1" title="Implement `init` Command">
        <subtask id="3.1.1">Create the file `capsule/commands/init.py`.</subtask>
        <subtask id="3.1.2">Implement the `init` command using `Typer` and `questionary` to prompt for user input.</subtask>
        <subtask id="3.1.3">Use the `Config` model to save the user's responses to the `config.yaml` file.</subtask>
        <subtask id="3.1.4">Add the new `init` command to the main CLI app in `capsule/cli.py`.</subtask>
      </task>
      <task id="3.2" title="Develop Unit Tests">
        <subtask id="3.2.1">Create the test file `tests/test_commands/test_init.py`.</subtask>
        <subtask id="3.2.2">Write a test that mocks the `questionary.prompt` function to simulate user input and verifies that the `config.yaml` is created with the correct content.</subtask>
        <subtask id="3.2.3">Write a test to simulate the user canceling the prompt and ensure the command exits gracefully without creating a file.</subtask>
      </task>
    </tasks>
  </story>

  <acceptanceCriteria>
      <criterion id="1">**`init` Command:** A new command `init` is added to the CLI in `capsule/commands/init.py`.</criterion>
      <criterion id="2">**Interactive Prompts:** The command uses the `questionary` library to interactively ask the user for their name, vault path, and API key.</criterion>
      <criterion id="3">**Configuration Creation:** Upon completion, a `config.yaml` file is created at `~/.capsule/config.yaml` with the user-provided values.</criterion>
      <criterion id="4">**Confirmation Message:** A confirmation message is displayed to the user indicating that the configuration has been saved.</criterion>
      <criterion id="5">**Graceful Exit:** The command handles cases where the user cancels the prompts (e.g., with Ctrl+C).</criterion>
      <criterion id="6">**Unit Tests:** The `init` command is tested in `tests/test_commands/test_init.py`, using mocks to simulate user input.</criterion>
    </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>Configuration Management</section>
        <snippet>
**Config File Initialization:**
```bash
# On first run: capsule init
? Vault path: /Users/bmad/Documents/Obsidian
? Research API key: ********************
? Default template: education
? Enable auto-backup: Yes

✓ Configuration saved to ~/.capsule/config.yaml
```
        </snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>CLI Command Structure Pattern</section>
        <snippet>
```python
# capsule/commands/generate.py
import typer
from rich.progress import Progress, SpinnerColumn, TextColumn
from capsule.core.generator import ContentGenerator
from capsule.utils.progress import create_progress_bar

app = typer.Typer()

@app.command()
def generate(
    # ... arguments ...
):
    """
    Generate a new capsule from research and templates.
    
    Example:
        capsule generate "Introduction to TCM Herbs" --template tcm
    """
    
    try:
        # ... command logic ...
    except Exception as e:
        # Consistent error handling
        typer.secho(f"❌ Error: {str(e)}", fg=typer.colors.RED, err=True)
        raise typer.Exit(code=1)
```
        </snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>capsule/commands/init.py</path>
        <kind>module</kind>
        <symbol>init</symbol>
        <lines></lines>
        <reason>This file needs to be created to house the new `init` command.</reason>
      </artifact>
      <artifact>
        <path>capsule/cli.py</path>
        <kind>module</kind>
        <symbol>app.add_typer(init_app, name="init")</symbol>
        <lines></lines>
        <reason>The new `init` command needs to be registered with the main Typer app.</reason>
      </artifact>
      <artifact>
        <path>tests/test_commands/test_init.py</path>
        <kind>test</kind>
        <symbol></symbol>
        <lines></lines>
        <reason>This test file needs to be created to provide comprehensive testing for the new command.</reason>
      </artifact>
    </code>
    <dependencies>
      <dependency>
        <ecosystem>Python</ecosystem>
        <package>questionary</package>
        <version>>=2.0.0</version>
      </dependency>
      <dependency>
        <ecosystem>Python</ecosystem>
        <package>Typer</package>
        <version>>=0.9.0</version>
      </dependency>
    </dependencies>
  </artifacts>

  <constraints>
      <constraint>The command must be named `init`.</constraint>
      <constraint>Interactive prompts must use the `questionary` library.</constraint>
      <constraint>The `Config` model must be used to save the configuration.</constraint>
    </constraints>
  <interfaces/>
  <tests>
      <standards>The project uses the `pytest` framework. Tests for commands are located in `tests/test_commands/`.</standards>
      <locations>
        <location>tests/test_commands/test_init.py</location>
      </locations>
      <ideas>
        <idea for="1">Test the successful creation of a `config.yaml` file with mocked user input.</idea>
        <idea for="2">Test that the command exits gracefully when the user cancels the prompts.</idea>
        <idea for="3">Test that the confirmation message is displayed correctly upon successful setup.</idea>
      </ideas>
    </tests>
</story-context>
